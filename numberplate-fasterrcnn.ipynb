{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bol036/teams/project-team-67\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kagglehub\n",
    "# from kagglehub import KaggleDatasetAdapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# car_plate_detection_path = kagglehub.dataset_download('andrewmvd/car-plate-detection')\n",
    "car_plate_detection_path = 'archive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'archive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      filename  width  height  depth object_name  xmin  ymin  xmax  ymax\n",
      "0  Cars175.png    600     450      3     licence   295   237   456   367\n",
      "1  Cars262.png    400     301      3     licence   243   184   285   205\n",
      "2   Cars86.png    586     331      3     licence   332   176   447   204\n",
      "3  Cars346.png    400     246      3     licence    91   185   197   217\n",
      "4  Cars397.png    400     299      3     licence   144   169   216   212\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Define paths\n",
    "annotation_folder = os.path.join(base_path, \"annotations\")\n",
    "\n",
    "# List to store data\n",
    "data = []\n",
    "\n",
    "# Function to parse XML and extract relevant information\n",
    "def parse_xml(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    filename = root.find(\"filename\").text\n",
    "    width = int(root.find(\".//size/width\").text)\n",
    "    height = int(root.find(\".//size/height\").text)\n",
    "    depth = int(root.find(\".//size/depth\").text)\n",
    "\n",
    "    obj = root.find(\".//object\")\n",
    "    obj_name = obj.find(\"name\").text\n",
    "    xmin = int(obj.find(\".//bndbox/xmin\").text)\n",
    "    ymin = int(obj.find(\".//bndbox/ymin\").text)\n",
    "    xmax = int(obj.find(\".//bndbox/xmax\").text)\n",
    "    ymax = int(obj.find(\".//bndbox/ymax\").text)\n",
    "\n",
    "    return [filename, width, height, depth, obj_name, xmin, ymin, xmax, ymax]\n",
    "\n",
    "# Process all annotation files\n",
    "for xml_file in os.listdir(annotation_folder):\n",
    "    if xml_file.endswith(\".xml\"):\n",
    "        xml_path = os.path.join(annotation_folder, xml_file)\n",
    "        data.append(parse_xml(xml_path))\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"filename\", \"width\", \"height\", \"depth\", \"object_name\", \"xmin\", \"ymin\", \"xmax\", \"ymax\"]\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Save to CSV\n",
    "# df.to_csv(os.path.join(base_path, \"annotations.csv\"), index=False)\n",
    "\n",
    "# Display DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['depth', 'object_name', 'height', 'width'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cars175.png</td>\n",
       "      <td>295</td>\n",
       "      <td>237</td>\n",
       "      <td>456</td>\n",
       "      <td>367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cars262.png</td>\n",
       "      <td>243</td>\n",
       "      <td>184</td>\n",
       "      <td>285</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cars86.png</td>\n",
       "      <td>332</td>\n",
       "      <td>176</td>\n",
       "      <td>447</td>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cars346.png</td>\n",
       "      <td>91</td>\n",
       "      <td>185</td>\n",
       "      <td>197</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cars397.png</td>\n",
       "      <td>144</td>\n",
       "      <td>169</td>\n",
       "      <td>216</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      filename  xmin  ymin  xmax  ymax\n",
       "0  Cars175.png   295   237   456   367\n",
       "1  Cars262.png   243   184   285   205\n",
       "2   Cars86.png   332   176   447   204\n",
       "3  Cars346.png    91   185   197   217\n",
       "4  Cars397.png   144   169   216   212"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('/kaggle/working/annotations.csv', index=False)\n",
    "df.to_csv('archive/annotations.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 21:36:37.041204: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-12-09 21:36:37.041261: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-12-09 21:36:37.042414: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-12-09 21:36:37.048152: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Custom Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumberPlateDataset(Dataset):\n",
    "    def __init__(self, csv_file, images_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            images_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get the row corresponding to the idx\n",
    "        row = self.data_frame.iloc[idx]\n",
    "        img_name = row['filename']  \n",
    "        \n",
    "        # Load image\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Get bounding box coordinates (e.g., x_min, y_min, x_max, y_max)\n",
    "        bbox = [row['xmin'], row['ymin'], row['xmax'], row['ymax']]\n",
    "        bbox = torch.tensor(bbox, dtype=torch.float32).unsqueeze(0)   #1 class\n",
    "\n",
    "        # For transfer learning, if you have only one class (number plate), you could use label 1.\n",
    "        # Adjust accordingly if you have more classes.\n",
    "        labels = torch.tensor([1], dtype=torch.int64)\n",
    "        \n",
    "        # Apply any transformations to the image (if provided)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        #target dictionary\n",
    "        target = {\n",
    "            'boxes': bbox,\n",
    "            'labels': labels\n",
    "        }\n",
    "        \n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths\n",
    "# base_path = '/kaggle/input/car-plate-detection'\n",
    "# csv_file = '/kaggle/working/annotations.csv'\n",
    "\n",
    "base_path = 'archive'\n",
    "csv_file = 'archive/annotations.csv'\n",
    "\n",
    "img_dir = os.path.join(base_path, \"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform for each image\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split and creating dataloader\n",
    "\n",
    "Made an 80-20 train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose dataset is an instance of your custom dataset class\n",
    "dataset = NumberPlateDataset(csv_file, img_dir, transform)\n",
    "\n",
    "# Define the split sizes\n",
    "train_size = int(0.8 * len(dataset))  # 80% for training\n",
    "test_size = len(dataset) - train_size  # 20% for testing\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=4, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=4, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([3, 450, 600])\n",
      "Bounding Boxes: tensor([[295., 237., 456., 367.]])\n",
      "Labels: tensor([1])\n"
     ]
    }
   ],
   "source": [
    "# Test dataset loading\n",
    "sample_image, sample_target = dataset[0]\n",
    "\n",
    "print(\"Image shape:\", sample_image.shape)  # Should be (3, H, W)\n",
    "print(\"Bounding Boxes:\", sample_target[\"boxes\"])  # Should be a tensor with shape (N, 4)\n",
    "print(\"Labels:\", sample_target[\"labels\"])  # Should be a tensor with 1 label (number plate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 4\n",
      "First image shape: torch.Size([3, 209, 400])\n",
      "First target boxes: tensor([[195., 149., 274., 209.]])\n"
     ]
    }
   ],
   "source": [
    "# Get a batch from DataLoader\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "print(\"Batch size:\", len(batch[0]))  # Should match batch_size (e.g., 4)\n",
    "print(\"First image shape:\", batch[0][0].shape)  # Should be (3, H, W)\n",
    "print(\"First target boxes:\", batch[1][0][\"boxes\"])  # Should be tensor([[xmin, ymin, xmax, ymax]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained Faster R-CNN ResNet50 FPN v2 model\n",
    "model = fasterrcnn_resnet50_fpn_v2(weights=\"DEFAULT\")\n",
    "#pretrained=True is for old versions so using weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model Architecture\n",
    "\n",
    "#### FasterRCNN with ResNet50 FPN V2\n",
    "* RCNN Transform\n",
    "* ResNet50 backbone with FPN\n",
    "  * Resnet50\n",
    "  * Feature Pyramid Network\n",
    "* Region Proposal Network\n",
    "  * Anchor Generator\n",
    "  * RPN Head\n",
    "* ROI Head\n",
    "  * Multi scale ROI align\n",
    "  * Fast RCNN ConvFC head\n",
    "  * Fast RCNN Predictor\n",
    "    * classification score -> should be 2 (1 for number plates 1 for background)\n",
    "    * bbox prediction -> coordinates of bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of parameters: 43712278\n",
      "Total number of trainable parameters: 43486934\n"
     ]
    }
   ],
   "source": [
    "# Count the total number of parameters in the model\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nTotal number of parameters: {total_params}\")\n",
    "print(f\"Total number of trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old predictor:\n",
      "FastRCNNPredictor(\n",
      "  (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
      "  (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print('Old predictor:')\n",
    "print(model.roi_heads.box_predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating the fastrcnn predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated ROI Head Predictor:\n",
      "FastRCNNPredictor(\n",
      "  (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get the number of input features for the classifier (the final fully connected layer)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# Define the number of classes (1 number plate + background)\n",
    "num_classes = 2\n",
    "\n",
    "# Replace the pre-trained predictor with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# Print model's ROI head to verify changes\n",
    "print(\"Updated ROI Head Predictor:\")\n",
    "print(model.roi_heads.box_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): FastRCNNConvFCHead(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Flatten(start_dim=1, end_dim=-1)\n",
       "      (5): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if CUDA is available and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "#transfers the model’s parameters to the selected device, ensuring all computations occur there\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freezing the backbone model\n",
    "\n",
    "will fine tune the rest of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the backbone (ResNet50 + FPN)\n",
    "for param in model.backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# can freeze the RPN later if needed:\n",
    "# for param in model.rpn.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# The ROI head, including the new box predictor, remains trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 43256153\n",
      "Number of trainable parameters: 16401689\n"
     ]
    }
   ],
   "source": [
    "# Calculate total number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# Calculate number of trainable parameters (i.e., those that require gradients)\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Total number of parameters:\", total_params)\n",
    "print(\"Number of trainable parameters:\", trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the optimizer and learning rate scheduler.\n",
    "# Only parameters with requires_grad=True will be updated.\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "scheduler = StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Helper Function to Compute IoU\n",
    "# -------------------------------\n",
    "def compute_iou(boxA, boxB):\n",
    "    \"\"\"\n",
    "    Computes Intersection over Union (IoU) between two bounding boxes.\n",
    "    Each box is defined as [x_min, y_min, x_max, y_max].\n",
    "    \"\"\"\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    \n",
    "    inter_width = max(0, xB - xA)\n",
    "    inter_height = max(0, yB - yA)\n",
    "    inter_area = inter_width * inter_height\n",
    "    \n",
    "    boxA_area = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    boxB_area = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "    \n",
    "    iou = inter_area / float(boxA_area + boxB_area - inter_area + 1e-6)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Train]:  37%|███▋      | 32/87 [00:22<00:38,  1.42batch/s, loss=0.2184]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     24\u001b[0m train_progress \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m [Train]\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_progress\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataset.py:399\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataset.py:399\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[0;32mIn[11], line 35\u001b[0m, in \u001b[0;36mNumberPlateDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Apply any transformations to the image (if provided)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m---> 35\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#target dictionary\u001b[39;00m\n\u001b[1;32m     38\u001b[0m target \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m'\u001b[39m: bbox,\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m: labels\n\u001b[1;32m     41\u001b[0m }\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torchvision/transforms/functional.py:173\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    171\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mview(pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m1\u001b[39m], pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m0\u001b[39m], F_pil\u001b[38;5;241m.\u001b[39mget_image_num_channels(pic))\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mdefault_float_dtype)\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Training Loop with Evaluation\n",
    "# -------------------------------\n",
    "# Define evaluation thresholds\n",
    "score_threshold = 0.5\n",
    "iou_threshold = 0.5\n",
    "\n",
    "num_epochs = 10\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Lists to store epoch-level metrics for plotting later\n",
    "epoch_train_losses = []\n",
    "epoch_val_losses = []\n",
    "epoch_val_avg_iou = []\n",
    "epoch_val_detection_rate = []  # Approximated as (TP / total images)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # ----- Training Phase -----\n",
    "    model.train()  # Ensure model is in training mode\n",
    "    train_metric_sums = defaultdict(float)\n",
    "    num_train_batches = 0\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    train_progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\", unit=\"batch\")\n",
    "    for images, targets in train_progress:\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        loss_dict = model(images, targets)  # Model returns a loss dictionary in training mode\n",
    "        total_loss = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        # Accumulate loss metrics\n",
    "        for key, loss in loss_dict.items():\n",
    "            train_metric_sums[key] += loss.item()\n",
    "        train_loss += total_loss.item()\n",
    "        num_train_batches += 1\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_progress.set_postfix(loss=f\"{total_loss.item():.4f}\")\n",
    "    \n",
    "    avg_train_loss = train_loss / num_train_batches\n",
    "    epoch_train_losses.append(avg_train_loss)\n",
    "    \n",
    "    print(f\"\\nEpoch [{epoch+1}/{num_epochs}] Training Loss: {avg_train_loss:.4f}\")\n",
    "    for metric, value in train_metric_sums.items():\n",
    "        print(f\"  {metric}: {value / num_train_batches:.4f}\")\n",
    "    \n",
    "    # ----- Validation Phase -----\n",
    "    # We need to compute both the loss and the detection metrics.\n",
    "    # For loss computation, we use training mode (which expects targets).\n",
    "    # For predictions, we temporarily switch to eval mode.\n",
    "    model.train()  # Remains in training mode for loss computation\n",
    "    val_metric_sums = defaultdict(float)\n",
    "    num_val_batches = 0\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    # For additional evaluation metrics:\n",
    "    total_iou = 0.0\n",
    "    total_images = 0\n",
    "    true_positive_count = 0  # Count images with a \"good\" detection\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_progress = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\", unit=\"batch\")\n",
    "        for images, targets in val_progress:\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            # Compute loss in training mode (expects targets)\n",
    "            loss_dict = model(images, targets)\n",
    "            total_loss_batch = sum(loss for loss in loss_dict.values())\n",
    "            val_loss += total_loss_batch.item()\n",
    "            num_val_batches += 1\n",
    "            \n",
    "            # Now, switch to eval mode temporarily to get predictions\n",
    "            model.eval()\n",
    "            outputs = model(images)  # In inference mode, outputs is a list of dicts (one per image)\n",
    "            model.train()  # Switch back to training mode for consistency\n",
    "            \n",
    "            # Evaluate detection performance for each image in the batch\n",
    "            for output, target in zip(outputs, targets):\n",
    "                # Assuming one ground truth box per image\n",
    "                gt_box = target[\"boxes\"][0].cpu().numpy()\n",
    "                pred_boxes = output[\"boxes\"].cpu().numpy()\n",
    "                pred_scores = output[\"scores\"].cpu().numpy()\n",
    "                if len(pred_boxes) > 0:\n",
    "                    max_index = pred_scores.argmax()\n",
    "                    pred_box = pred_boxes[max_index]\n",
    "                    pred_score = pred_scores[max_index]\n",
    "                    iou = compute_iou(pred_box, gt_box)\n",
    "                else:\n",
    "                    iou = 0.0\n",
    "                    pred_score = 0.0\n",
    "                total_iou += iou\n",
    "                total_images += 1\n",
    "                # Count as a true positive if thresholds are met\n",
    "                if pred_score >= score_threshold and iou >= iou_threshold:\n",
    "                    true_positive_count += 1\n",
    "                    \n",
    "            val_progress.set_postfix(loss=f\"{total_loss_batch.item():.4f}\")\n",
    "    \n",
    "    avg_val_loss = val_loss / num_val_batches\n",
    "    epoch_val_losses.append(avg_val_loss)\n",
    "    \n",
    "    # Compute average IoU and detection rate (as an approximation for AP)\n",
    "    avg_iou = total_iou / total_images if total_images > 0 else 0.0\n",
    "    detection_rate = true_positive_count / total_images if total_images > 0 else 0.0\n",
    "    \n",
    "    epoch_val_avg_iou.append(avg_iou)\n",
    "    epoch_val_detection_rate.append(detection_rate)\n",
    "    \n",
    "    print(f\"\\nEpoch [{epoch+1}/{num_epochs}] Validation Loss: {avg_val_loss:.4f}\")\n",
    "    for metric, value in val_metric_sums.items():\n",
    "        print(f\"  {metric}: {value / num_val_batches:.4f}\")\n",
    "    print(f\"  Average IoU: {avg_iou:.4f}\")\n",
    "    print(f\"  Detection Rate: {detection_rate:.4f}\")\n",
    "    \n",
    "    # Save best model based on validation loss\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        print(\"Saved best model.\")\n",
    "    \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Plotting Metrics After Training\n",
    "# -------------------------------\n",
    "epochs = range(1, num_epochs + 1)\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot Losses\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, epoch_train_losses, 'b-o', label='Train Loss')\n",
    "plt.plot(epochs, epoch_val_losses, 'r-o', label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot IoU and Detection Rate\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, epoch_val_avg_iou, 'g-o', label='Avg IoU')\n",
    "plt.plot(epochs, epoch_val_detection_rate, 'm-o', label='Detection Rate')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metric')\n",
    "plt.title('Validation IoU & Detection Rate Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on random images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of random images to visualize\n",
    "num_samples = 10\n",
    "# Randomly sample indices from the validation dataset\n",
    "sample_indices = random.sample(range(len(test_dataset)), num_samples)\n",
    "\n",
    "# Set model to evaluation mode for inference\n",
    "model.eval()\n",
    "\n",
    "for idx in sample_indices:\n",
    "    # Get image and ground truth target from the validation dataset\n",
    "    image, target = test_dataset[idx]\n",
    "    \n",
    "    # If image is a tensor, convert it to a PIL image for display\n",
    "    if torch.is_tensor(image):\n",
    "        # Assuming the tensor is in [0,1] range and of shape (C, H, W)\n",
    "        image_disp = F.to_pil_image(image)\n",
    "    else:\n",
    "        image_disp = image\n",
    "\n",
    "    # Run inference on a single image; model expects a list of images\n",
    "    with torch.no_grad():\n",
    "        output = model([image.to(device)])[0]  # output is a dict for this image\n",
    "\n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(1, figsize=(10, 6))\n",
    "    ax.imshow(image_disp)\n",
    "\n",
    "    # Plot ground truth bounding box (in green)\n",
    "    gt_box = target[\"boxes\"][0].cpu().numpy()  # Assuming one ground truth box per image\n",
    "    gt_rect = plt.Rectangle((gt_box[0], gt_box[1]),\n",
    "                            gt_box[2] - gt_box[0],\n",
    "                            gt_box[3] - gt_box[1],\n",
    "                            fill=False, edgecolor='green', linewidth=2, label='Ground Truth')\n",
    "    ax.add_patch(gt_rect)\n",
    "\n",
    "    # Plot predicted bounding boxes (in red) for predictions with a score above a threshold (e.g., 0.5)\n",
    "    pred_boxes = output[\"boxes\"].cpu().numpy()\n",
    "    pred_scores = output[\"scores\"].cpu().numpy()\n",
    "    for box, score in zip(pred_boxes, pred_scores):\n",
    "        if score < 0.5:\n",
    "            continue  # Skip predictions with low confidence\n",
    "        pred_rect = plt.Rectangle((box[0], box[1]),\n",
    "                                  box[2] - box[0],\n",
    "                                  box[3] - box[1],\n",
    "                                  fill=False, edgecolor='red', linewidth=2, label='Prediction')\n",
    "        ax.add_patch(pred_rect)\n",
    "\n",
    "    # Remove duplicate legend entries\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    unique = dict(zip(labels, handles))\n",
    "    ax.legend(unique.values(), unique.keys())\n",
    "\n",
    "    plt.title(f\"Validation Image Index: {idx}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"final_model.pth\", _use_new_zipfile_serialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simplified License Plate Recognition Code - Ready to Add to Notebook\n",
    "\n",
    "Just 3 Steps:\n",
    "1. Install EasyOCR\n",
    "2. Copy this code\n",
    "3. Run and test\n",
    "\"\"\"\n",
    "\n",
    "# ============================================\n",
    "# Step 1: Install Dependencies (Run in Notebook)\n",
    "# ============================================\n",
    "# !pip install easyocr\n",
    "\n",
    "# ============================================\n",
    "# Step 2: Import Libraries and Initialize OCR\n",
    "# ============================================\n",
    "import easyocr\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Initialize OCR reader (will download model on first run)\n",
    "print(\"Initializing OCR reader...\")\n",
    "reader = easyocr.Reader(['ch_sim', 'en'], gpu=True)  # Chinese + English\n",
    "print(\"Initialization complete!\")\n",
    "\n",
    "# ============================================\n",
    "# Step 3: Define Recognition Functions\n",
    "# ============================================\n",
    "\n",
    "def recognize_license_plate(image_path, model, device, transform, confidence_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Complete license plate recognition pipeline\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to image file\n",
    "        model: Trained Faster R-CNN model\n",
    "        device: 'cuda' or 'cpu'\n",
    "        transform: Image transformation\n",
    "        confidence_threshold: Detection confidence threshold\n",
    "    \n",
    "    Returns:\n",
    "        results: List of recognition results\n",
    "        annotated_image: Image with annotations\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Cannot read image {image_path}\")\n",
    "        return [], None\n",
    "    \n",
    "    # Prepare model input\n",
    "    img_pil = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    img_tensor = transform(img_pil).unsqueeze(0).to(device)\n",
    "    \n",
    "    # License plate detection\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(img_tensor)\n",
    "    \n",
    "    # Get detection results\n",
    "    boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "    scores = predictions[0]['scores'].cpu().numpy()\n",
    "    \n",
    "    # Store recognition results\n",
    "    results = []\n",
    "    \n",
    "    # Process each detected license plate\n",
    "    for idx, (box, score) in enumerate(zip(boxes, scores)):\n",
    "        if score > confidence_threshold:\n",
    "            # Crop license plate region\n",
    "            xmin, ymin, xmax, ymax = map(int, box)\n",
    "            \n",
    "            # Ensure coordinates are within bounds\n",
    "            h, w = image.shape[:2]\n",
    "            xmin = max(0, xmin)\n",
    "            ymin = max(0, ymin)\n",
    "            xmax = min(w, xmax)\n",
    "            ymax = min(h, ymax)\n",
    "            \n",
    "            plate_img = image[ymin:ymax, xmin:xmax]\n",
    "            \n",
    "            # Skip regions that are too small\n",
    "            if plate_img.shape[0] < 10 or plate_img.shape[1] < 10:\n",
    "                continue\n",
    "            \n",
    "            # OCR recognition\n",
    "            try:\n",
    "                ocr_results = reader.readtext(plate_img)\n",
    "                \n",
    "                if ocr_results:\n",
    "                    # Get result with highest confidence\n",
    "                    best_result = max(ocr_results, key=lambda x: x[2])\n",
    "                    plate_text = best_result[1].replace(\" \", \"\").replace(\"-\", \"\")\n",
    "                    ocr_confidence = best_result[2]\n",
    "                    \n",
    "                    # Save result\n",
    "                    results.append({\n",
    "                        'plate_number': plate_text,\n",
    "                        'detection_score': float(score),\n",
    "                        'ocr_confidence': float(ocr_confidence),\n",
    "                        'bbox': box.tolist()\n",
    "                    })\n",
    "                    \n",
    "                    # Draw bounding box and text on image\n",
    "                    cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (0, 255, 0), 3)\n",
    "                    \n",
    "                    # Add text label\n",
    "                    label = f\"{plate_text} ({score:.2f})\"\n",
    "                    \n",
    "                    # Calculate text background\n",
    "                    (text_w, text_h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.8, 2)\n",
    "                    cv2.rectangle(image, (xmin, ymin - text_h - 10), \n",
    "                                (xmin + text_w, ymin), (0, 255, 0), -1)\n",
    "                    \n",
    "                    # Add text\n",
    "                    cv2.putText(image, label, (xmin, ymin - 5),\n",
    "                              cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2)\n",
    "                    \n",
    "                    print(f\"Plate #{idx+1}: {plate_text} (Detection: {score:.3f}, OCR: {ocr_confidence:.3f})\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error recognizing plate #{idx+1}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return results, image\n",
    "\n",
    "\n",
    "def batch_recognize(image_paths, model, device, transform, confidence_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Batch recognition for multiple images\n",
    "    \n",
    "    Args:\n",
    "        image_paths: List of image file paths\n",
    "        Other args same as recognize_license_plate\n",
    "    \n",
    "    Returns:\n",
    "        all_results: Recognition results for all images\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    for idx, img_path in enumerate(image_paths):\n",
    "        print(f\"\\nProcessing image {idx+1}/{len(image_paths)}: {img_path}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        results, annotated_img = recognize_license_plate(\n",
    "            img_path, model, device, transform, confidence_threshold\n",
    "        )\n",
    "        \n",
    "        # Save results for each image\n",
    "        for result in results:\n",
    "            result['image_path'] = img_path\n",
    "            all_results.append(result)\n",
    "        \n",
    "        # Display result image\n",
    "        if annotated_img is not None:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.imshow(cv2.cvtColor(annotated_img, cv2.COLOR_BGR2RGB))\n",
    "            plt.axis('off')\n",
    "            plt.title(f'Recognition Result: {img_path}')\n",
    "            plt.show()\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Step 4: Usage Examples\n",
    "# ============================================\n",
    "\n",
    "# Example 1: Recognize single image\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Example 1: Single Image Recognition\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_image = os.path.join(base_path, 'images/Cars0.png')\n",
    "results, result_image = recognize_license_plate(\n",
    "    test_image, \n",
    "    model,  # Your trained model\n",
    "    device, \n",
    "    transform,\n",
    "    confidence_threshold=0.5\n",
    ")\n",
    "\n",
    "# Display results\n",
    "if result_image is not None:\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.imshow(cv2.cvtColor(result_image, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.title('License Plate Recognition Result', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Print recognition results\n",
    "print(\"\\nRecognition Results:\")\n",
    "print(\"-\" * 50)\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Plate {i+1}:\")\n",
    "    print(f\"  Plate Number: {result['plate_number']}\")\n",
    "    print(f\"  Detection Confidence: {result['detection_score']:.3f}\")\n",
    "    print(f\"  OCR Confidence: {result['ocr_confidence']:.3f}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "# Example 2: Batch recognition for multiple images\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Example 2: Batch Recognition\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get all test images\n",
    "image_folder = os.path.join(base_path, 'images')\n",
    "test_images = [os.path.join(image_folder, f) for f in os.listdir(image_folder)[:5]]  # Test first 5 images\n",
    "\n",
    "all_results = batch_recognize(\n",
    "    test_images,\n",
    "    model,\n",
    "    device,\n",
    "    transform,\n",
    "    confidence_threshold=0.5\n",
    ")\n",
    "\n",
    "# Save results to CSV\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(all_results)\n",
    "df.to_csv('license_plate_results.csv', index=False)\n",
    "print(f\"\\nAll results saved to: license_plate_results.csv\")\n",
    "print(f\"Total {len(all_results)} plates recognized\")\n",
    "\n",
    "\n",
    "# Example 3: Statistical analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Example 3: Recognition Statistics\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if len(all_results) > 0:\n",
    "    avg_detection = sum(r['detection_score'] for r in all_results) / len(all_results)\n",
    "    avg_ocr = sum(r['ocr_confidence'] for r in all_results) / len(all_results)\n",
    "    \n",
    "    print(f\"Total Recognitions: {len(all_results)}\")\n",
    "    print(f\"Average Detection Confidence: {avg_detection:.3f}\")\n",
    "    print(f\"Average OCR Confidence: {avg_ocr:.3f}\")\n",
    "    \n",
    "    # Display all recognized plate numbers\n",
    "    print(\"\\nAll Recognized Plate Numbers:\")\n",
    "    for i, result in enumerate(all_results):\n",
    "        print(f\"  {i+1}. {result['plate_number']}\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Additional Feature: Image Preprocessing (Improve Accuracy)\n",
    "# ============================================\n",
    "\n",
    "def preprocess_plate_image(plate_img):\n",
    "    \"\"\"\n",
    "    Preprocess cropped plate image to improve recognition accuracy\n",
    "    \"\"\"\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(plate_img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Denoise\n",
    "    denoised = cv2.fastNlMeansDenoising(gray, h=10)\n",
    "    \n",
    "    # Adaptive thresholding\n",
    "    binary = cv2.adaptiveThreshold(\n",
    "        denoised, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "        cv2.THRESH_BINARY, 11, 2\n",
    "    )\n",
    "    \n",
    "    # Morphological operations (remove small noise)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "    cleaned = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "\n",
    "# If you need to use preprocessing, modify this line in the recognition function:\n",
    "# plate_img = preprocess_plate_image(plate_img)\n",
    "# ocr_results = reader.readtext(plate_img)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"All functions loaded successfully!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nUsage Instructions:\")\n",
    "print(\"1. Single recognition: recognize_license_plate(image_path, model, device, transform)\")\n",
    "print(\"2. Batch recognition: batch_recognize(image_paths, model, device, transform)\")\n",
    "print(\"3. Image preprocessing: preprocess_plate_image(plate_img)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 686454,
     "sourceId": 1203932,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (clean)",
   "language": "python",
   "name": "python3_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
